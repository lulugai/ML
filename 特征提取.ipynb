{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import xgboost\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression,Lasso,LassoCV, LassoLarsCV, LassoLarsIC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, AdaBoostClassifier,VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV,KFold, cross_val_predict,StratifiedKFold,train_test_split,cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,plot_confusion_matrix,auc,plot_roc_curve,ConfusionMatrixDisplay,mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import missingno as msno\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "RANDOM_NUM = 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#导入数据\n",
    "df = pd.read_excel(\"./features_total.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#异常值处理\n",
    "def outliers(x):   #传入某变量  \n",
    "    mean_value = x.mean()   #计算该变量的均值  \n",
    "    std_value = x.std()   #计算该变量的标准差  \n",
    "    rule = (mean_value - 3 * std_value > x) | (x.mean() + 3 * x.std() < x)   #处于(mean-3std,mean+3std)区间外的数据为异常值\n",
    "    index = np.arange(x.shape[0])[rule]   #获取异常值的行位置索引  \n",
    "    x = x.replace(x.iloc[index],nan,inplace=True)   #获取异常值的数据 \n",
    "    return x   #返回异常值的数据\n",
    "\n",
    "for i in range(117):\n",
    "    outliers(df[df.columns[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#缺失值填充\n",
    "df_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df_miss = pd.DataFrame(df_mean.fit_transform(df),columns=df.columns)\n",
    "df_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#划分数据和标签\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=RANDOM_NUM, stratify=df['label'])\n",
    "\n",
    "df_miss = df_train\n",
    "target=df_miss[list(df_miss.columns)[2:3]]\n",
    "features=df_miss[list(df_miss.columns)[3:]]\n",
    "#数据标准化\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(features)\n",
    "X_train=pd.DataFrame(X_train,columns=df.columns[3:])\n",
    "y_train = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有数据分类十折交叉验证\n",
    "def Kfold_model_confusion(model, X, y, name):\n",
    "    cv = StratifiedKFold(n_splits=10,shuffle=True,random_state=1234)\n",
    "    accuracy = cross_val_score(model,X, y, cv=cv,scoring=\"accuracy\",n_jobs=-1)\n",
    "    auc = cross_val_score(model,X, y, cv=cv,scoring=\"roc_auc\",n_jobs=-1)\n",
    "    recall = cross_val_score(model,X, y, cv=cv,scoring=\"recall\",n_jobs=-1)\n",
    "    precision = cross_val_score(model,X, y, cv=cv,scoring=\"precision\",n_jobs=-1)\n",
    "    f1 = cross_val_score(model,X, y, cv=cv,scoring=\"f1\",n_jobs=-1)\n",
    "    result_dict = {'Accuracy': [accuracy.mean()], \n",
    "                  'Auc': [auc.mean()], \n",
    "                  'Recall': [recall.mean()], \n",
    "                  'Precision': [precision.mean()],\n",
    "                  'F1 score': [f1.mean()]}\n",
    "    result = pd.DataFrame(result_dict, index=[name])\n",
    "    return result\n",
    "\n",
    "#创建分类器\n",
    "lr_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "forest_clf = RandomForestClassifier(random_state=0,n_jobs=-1,class_weight=\"balanced\")\n",
    "gra_clf = GradientBoostingClassifier(n_estimators=500)\n",
    "ada_clf = AdaBoostClassifier(n_estimators=500)\n",
    "dt_clf = DecisionTreeClassifier(random_state=1234)\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
    "xgb_clf = xgboost.XGBClassifier(n_estimators=500, learning_rate=0.2, \n",
    "                                gamma=0.5, max_depth=20, verbosity=0)\n",
    "en_clf = VotingClassifier(estimators=[('rf', forest_clf), ('gb', gra_clf), ('xgb', xgb_clf), ('lgbm', lgbm_clf)],\n",
    "                         voting='soft',weights=[4, 3, 4, 5])\n",
    "\n",
    "\n",
    "#训练分类器并输出结果\n",
    "result3 = Kfold_model_confusion(lr_clf, X_train, y_train, 'Logistic Regression')\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(knn_clf, X_train, y_train, 'KNN')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(svm_clf, X_train, y_train, 'SVM')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(forest_clf, X_train, y_train, 'Random Forest')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(gra_clf, X_train, y_train, 'Gradient Boosting')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(ada_clf, X_train, y_train, 'AdaBoosting')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(dt_clf, X_train, y_train, 'Decision Tree')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(lgbm_clf,X_train, y_train, 'LGBM')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(xgb_clf, X_train, y_train, 'XGBoost')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(en_clf,X_train, y_train, 'Ensemble')])\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混淆矩阵\n",
    "clf=en_clf\n",
    "cons = 0\n",
    "#target_ = np.array(target, dtype = int)\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True,random_state=1)\n",
    "for i, (train, test) in enumerate(cv.split(X_train, y_train)):\n",
    "    clf.fit(X_train.iloc[train], y_train.iloc[train])\n",
    "    class_names=['0','1']\n",
    "    a=confusion_matrix(y_train.iloc[test],(clf.fit(X_train.iloc[train], y_train.iloc[train])).predict(X_train.iloc[test]))\n",
    "    cons=cons+a\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cons,display_labels=class_names)\n",
    "disp.plot(cmap = 'Blues')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10,shuffle=True,random_state=1)\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()   \n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "for i, (train, test) in enumerate(cv.split(X_train, y_train)):\n",
    "    clf.fit(X_train.iloc[train], y_train.iloc[train])\n",
    "    viz = plot_roc_curve(clf, X_train.iloc[test],y_train.iloc[test])\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2,color='r', \n",
    "         alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "ax.plot(mean_fpr, mean_tpr, \n",
    "        label=r'AUC = %0.2f'% (mean_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EPSILON = 1e-4\n",
    "# LassoCV: coordinate descent\n",
    "\n",
    "# Compute paths\n",
    "print(\"Computing regularization path using the coordinate descent lasso...\")\n",
    "t1 = time.time()\n",
    "model = LassoCV(cv=10).fit(X_train, y_train)\n",
    "t_lasso_cv = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "plt.figure()\n",
    "#设置字体大小\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "ymin, ymax = 2300, 3800\n",
    "plt.semilogx(model.alphas_ + EPSILON, model.mse_path_, \":\")\n",
    "plt.plot(\n",
    "    model.alphas_ + EPSILON,\n",
    "    model.mse_path_.mean(axis=-1),\n",
    "    \"k\",\n",
    "    label=\"Average across the folds\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.axvline(\n",
    "    model.alpha_ + EPSILON, linestyle=\"--\", color=\"k\", label=\"alpha: CV estimate\"\n",
    ")\n",
    "print(model.alpha_+ EPSILON)\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel(\"Mean square error\")\n",
    "plt.title(\n",
    "    \"Mean square error on each fold\"\n",
    "    \n",
    ")\n",
    "plt.axis(\"tight\")\n",
    "#plt.ylim(ymin, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = Lasso()\n",
    "coeficents = []\n",
    "errors_train = []\n",
    "#errors_test = []\n",
    "\n",
    "alphas = np.logspace(-5, 5, 200)\n",
    "for a in alphas:\n",
    "    model_lasso.set_params(alpha=a)\n",
    "    model_lasso.fit(X_train, y_train)\n",
    "    coeficents.append(model_lasso.coef_)\n",
    "    errors_train.append(mean_squared_error(y_train, model_lasso.predict(X_train)))\n",
    "#    errors_test.append(mean_squared_error(diabetes_y_test,model_lasso.predict(diabetes_X_test)))\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "plt.subplot(121)\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coeficents)\n",
    "ax.set_xscale('log')\n",
    "plt.axvline(model.alpha_ + EPSILON, linestyle=\"--\", color=\"k\", label=\"alpha: CV estimate\")\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso Coeficients (training)')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.subplot(122)\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, errors_train,linestyle=\"-\", label=\"Train\")\n",
    "#ax.plot(alphas, errors_test,linestyle=\"--\", label=\"Test\")\n",
    "ax.set_xscale('log')\n",
    "plt.axvline(model.alpha_ + EPSILON, linestyle=\"--\", color=\"k\", label=\"alpha: CV estimate\")\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Lasso - Training Errors')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "feature_selection= SelectFromModel(Lasso(alpha=model.alpha_,random_state=RANDOM_NUM)) \n",
    "feature_selection.fit(X_train,y_train)\n",
    "selected_feat = X_train.columns[(feature_selection.get_support())]\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.sans-serif'] = ['SimHei'] \n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "plt.figure(figsize=(10,14))\n",
    "model_lasso = Lasso(alpha = model.alpha_, random_state=RANDOM_NUM).fit(X_train, y_train)\n",
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
    "imp_coef = pd.concat([coef.sort_values().head(24),\n",
    "                     coef.sort_values().tail(24)])\n",
    "imp_coef.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df = df.loc[:, selected_feat]\n",
    "select_df = pd.concat([df[list(df.columns)[:3]], select_df], axis= 1)\n",
    "display(select_df)\n",
    "select_df.to_csv('./Selected_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lasso=X_train.loc[:, selected_feat]\n",
    "X_train_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有数据分类十折交叉验证\n",
    "def Kfold_model_confusion(model, X, y, name):\n",
    "    cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\n",
    "    accuracy = cross_val_score(model,X, y, cv=cv,scoring=\"accuracy\",n_jobs=-1)\n",
    "    auc = cross_val_score(model,X, y, cv=cv,scoring=\"roc_auc\",n_jobs=-1)\n",
    "    recall = cross_val_score(model,X, y, cv=cv,scoring=\"recall\",n_jobs=-1)\n",
    "    precision = cross_val_score(model,X, y, cv=cv,scoring=\"precision\",n_jobs=-1)\n",
    "    f1 = cross_val_score(model,X, y, cv=cv,scoring=\"f1\",n_jobs=-1)\n",
    "    result_dict = {'Accuracy': [accuracy.mean()], \n",
    "                  'Auc': [auc.mean()], \n",
    "                  'Recall': [recall.mean()], \n",
    "                  'Precision': [precision.mean()],\n",
    "                  'F1 score': [f1.mean()]}\n",
    "    result = pd.DataFrame(result_dict, index=[name])\n",
    "    return result\n",
    "\n",
    "#创建分类器\n",
    "lr_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "forest_clf = RandomForestClassifier(random_state=0,n_jobs=-1,class_weight=\"balanced\")\n",
    "gra_clf = GradientBoostingClassifier(n_estimators=500)\n",
    "ada_clf = AdaBoostClassifier(n_estimators=500)\n",
    "dt_clf = DecisionTreeClassifier(random_state=1234)\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
    "xgb_clf = xgboost.XGBClassifier(n_estimators=500, learning_rate=0.2, \n",
    "                                gamma=0.5, max_depth=20, verbosity=0)\n",
    "en_clf = VotingClassifier(estimators=[('rf', forest_clf), ('gb', gra_clf), ('xgb', xgb_clf), ('lgbm', lgbm_clf)],\n",
    "                         voting='soft',weights=[4, 3, 4, 5])\n",
    "\n",
    "\n",
    "#训练分类器并输出结果\n",
    "result3 = Kfold_model_confusion(lr_clf, X_train_lasso, y_train, 'Logistic Regression')\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(knn_clf, X_train_lasso, y_train, 'KNN')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(svm_clf, X_train_lasso, y_train, 'SVM')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(forest_clf, X_train_lasso, y_train, 'Random Forest')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(gra_clf, X_train_lasso, y_train, 'Gradient Boosting')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(ada_clf, X_train_lasso, y_train, 'AdaBoosting')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(dt_clf, X_train_lasso, y_train, 'Decision Tree')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(lgbm_clf,X_train_lasso, y_train, 'LGBM')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(xgb_clf, X_train_lasso, y_train, 'XGBoost')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(en_clf,X_train_lasso, y_train, 'Ensemble')])\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混淆矩阵\n",
    "clf=en_clf\n",
    "cons = 0\n",
    "#target_ = np.array(target, dtype = int)\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "for i, (train, test) in enumerate(cv.split(X_train_lasso, y_train)):\n",
    "    clf.fit(X_train_lasso.iloc[train], y_train.iloc[train])\n",
    "    class_names=['0','1']\n",
    "    a=confusion_matrix(y_train.iloc[test],(clf.fit(X_train_lasso.iloc[train], y_train.iloc[train])).predict(X_train_lasso.iloc[test]))\n",
    "    cons=cons+a\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cons,display_labels=class_names)\n",
    "disp.plot(cmap = 'Blues')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10,shuffle=True,random_state=1)\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()   \n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "for i, (train, test) in enumerate(cv.split(X_train_lasso, y_train)):\n",
    "    clf.fit(X_train_lasso.iloc[train], y_train.iloc[train])\n",
    "    viz = plot_roc_curve(clf, X_train_lasso.iloc[test],y_train.iloc[test])\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2,color='r', \n",
    "         alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "ax.plot(mean_fpr, mean_tpr, \n",
    "        label=r'AUC = %0.2f'% (mean_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#用selectbest对特征做方差分析F检验\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2,f_classif\n",
    "model = SelectKBest(f_classif, k=44)\n",
    "select_feature=model.fit_transform(X_train_lasso,y_train)\n",
    "scores=model.pvalues_\n",
    "indices=np.argsort(scores)[::-1]\n",
    "print('Features ANOVA p_value')\n",
    "#\"%0.2f%s\" % (scores[indices[i]],\n",
    "for i in range(len(scores)):\n",
    "    print(\"'\"+X_train_lasso.columns[indices[i]]+\"'\"+\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_feature=['月经间隔天数',\n",
    "'嗜碱性粒细胞绝对值',\n",
    "'高血压史',\n",
    "'谷酰转肽酶',\n",
    "'睡眠时间',\n",
    "'晚睡',\n",
    "'血糖值',\n",
    "'是否绝经',\n",
    "'WHR',\n",
    "'臀围',\n",
    "'胡萝卜',\n",
    "'运动量',\n",
    "'白蛋白',\n",
    "'白蛋白球蛋白比',\n",
    "'噩梦',\n",
    "'淋巴细胞比例',\n",
    "'肌酐',\n",
    "'中性粒细胞比例',\n",
    "'失眠',\n",
    "'空腹血糖.葡萄糖.',\n",
    "'乳腺增生',\n",
    "'二手烟',\n",
    "'中性粒细胞绝对值',\n",
    "'早醒',\n",
    "'怀孕次数',\n",
    "'母乳喂养月数',\n",
    "'奶制品',\n",
    "'孩子个数',\n",
    "'城市/农村ur',\n",
    "'生活满意度',\n",
    "'经济状况',\n",
    "'家庭月均收入',\n",
    "'行为预防得分']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_imp=X_train.loc[:, select_feature]\n",
    "X_train_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有数据分类十折交叉验证\n",
    "def Kfold_model_confusion(model, X, y, name):\n",
    "    cv = StratifiedKFold(n_splits=10,shuffle=True,random_state=1234)\n",
    "    accuracy = cross_val_score(model,X, y, cv=cv,scoring=\"accuracy\",n_jobs=-1)\n",
    "    auc = cross_val_score(model,X, y, cv=cv,scoring=\"roc_auc\",n_jobs=-1)\n",
    "    recall = cross_val_score(model,X, y, cv=cv,scoring=\"recall\",n_jobs=-1)\n",
    "    precision = cross_val_score(model,X, y, cv=cv,scoring=\"precision\",n_jobs=-1)\n",
    "    f1 = cross_val_score(model,X, y, cv=cv,scoring=\"f1\",n_jobs=-1)\n",
    "    result_dict = {'Accuracy': [accuracy.mean()], \n",
    "                  'Auc': [auc.mean()], \n",
    "                  'Recall': [recall.mean()], \n",
    "                  'Precision': [precision.mean()],\n",
    "                  'F1 score': [f1.mean()]}\n",
    "    result = pd.DataFrame(result_dict, index=[name])\n",
    "    return result\n",
    "\n",
    "#创建分类器\n",
    "lr_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "forest_clf = RandomForestClassifier(random_state=0,n_jobs=-1,class_weight=\"balanced\")\n",
    "gra_clf = GradientBoostingClassifier(n_estimators=500)\n",
    "ada_clf = AdaBoostClassifier(n_estimators=500)\n",
    "dt_clf = DecisionTreeClassifier(random_state=1234)\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
    "xgb_clf = xgboost.XGBClassifier(n_estimators=500, learning_rate=0.2, \n",
    "                                gamma=0.5, max_depth=20, verbosity=0)\n",
    "en_clf = VotingClassifier(estimators=[('rf', forest_clf), ('gb', gra_clf), ('xgb', xgb_clf), ('lgbm', lgbm_clf)],\n",
    "                         voting='soft',weights=[4, 3, 4, 5])\n",
    "\n",
    "\n",
    "#训练分类器并输出结果\n",
    "result3 = Kfold_model_confusion(lr_clf, X_train_imp, y_train, 'Logistic Regression')\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(knn_clf, X_train_imp, y_train, 'KNN')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(svm_clf, X_train_imp, y_train, 'SVM')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(forest_clf, X_train_imp, y_train, 'Random Forest')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(gra_clf, X_train_imp, y_train, 'Gradient Boosting')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(ada_clf, X_train_imp, y_train, 'AdaBoosting')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(dt_clf, X_train_imp, y_train, 'Decision Tree')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(lgbm_clf,X_train_imp, y_train, 'LGBM')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(xgb_clf, X_train_imp, y_train, 'XGBoost')])\n",
    "result3 = pd.concat([result3, Kfold_model_confusion(en_clf,X_train_imp, y_train, 'Ensemble')])\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
